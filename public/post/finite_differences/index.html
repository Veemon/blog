<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Finite_differences | Russel Demos blog</title><meta name=keywords content="pde,numeric"><meta name=description content="... Description ..."><meta name=author content><link rel=canonical href=/post/finite_differences/><link crossorigin=anonymous href=/assets/css/stylesheet.0499addc25d0d00f38d3c7ff2f1c44dd39b476d8d6f8bc259e35eb4c0593028c.css integrity="sha256-BJmt3CXQ0A8408f/LxxE3Tm0dtjW+LwlnjXrTAWTAow=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><head><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script></head><meta property="og:title" content="Finite_differences"><meta property="og:description" content="... Description ..."><meta property="og:type" content="article"><meta property="og:url" content="/post/finite_differences/"><meta property="article:section" content="post"><meta property="article:published_time" content="2020-11-15T00:00:00+00:00"><meta property="article:modified_time" content="2020-11-15T00:00:00+00:00"><meta property="og:site_name" content="Russel Demos blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Finite_differences"><meta name=twitter:description content="... Description ..."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"/post/"},{"@type":"ListItem","position":3,"name":"Finite_differences","item":"/post/finite_differences/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Finite_differences","name":"Finite_differences","description":"... Description ...","keywords":["pde","numeric"],"articleBody":"For some context behind this post, I had just completed my first year in mathematics and was inspired by the PDE's part of a differential equations class I took. At this point, I had not taken analysis however I had read Spivak's book the year prior. Nonetheless, I was _excited_ to be doing math after a terrible first year, and I still do think about \"simulation for artistic recreation\", as I will say in the upcoming motivation. It's a bit crazy to think how much I would encounter and learn in the years following this post ... - Russel, June 28, 2023. Can we apply deep learning to PDE’s? Motivation I am interested in large-scale simulation for artistic recreation. I find a lot of the patterns and chaotic dynamics found in the natural world quite beautiful, as I’m sure most people do. In a sort of “aesthetically”-driven pursuit of replicating this, I hope I can learn more about a wide array of fields and potentially produce novel results.\nAlthough in general I wish to make use of geo-physical simulation for planet-scale landform generation, in this work I am particularly building up to modelling mantle convection and the interactions between the mantle and the lithosphere. With such results, as I wish to create a pseudo-real-time simulation, I will look into approximation methods that utilize modern techniques from deep learning.\n1. Recap of Numerical Differentiation 1.1 First Order - Single Variable There are many ways to calculate the slope of the secant line at a point $x$. $$ f’(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h} \\tag*{(Forward Difference)} $$\n$$ f’(x) = \\lim_{h \\to 0} \\frac{f(x) - f(x-h)}{h} \\tag*{(Backward Difference)} $$\n$$ f’(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x-h)}{2h} \\tag*{(Centered Difference)} $$\nOfcourse, for the numerical approach instead of a continuum of values for a function $f$, we have the descretization $f_i$ over the ordered set of sample points $i \\in N$. Each $f_i$ is thus sampled a distance of $\\Delta x = \\frac{1}{n} (R_{max} - R_{min})$ away from the last point, over a domain region $[R_{min}, R_{max}]$.\nRecall the Taylor Series for a function $f$, evaluated at some $a$ for a given $x_0$, $$ f(x_0) \\approx \\sum_{n=0}^\\infty \\frac{(x_0 - a)^n}{n!} \\cdot f^{(n)}(a) $$\nIf we were to take the forward difference approach, consider taking the Taylor series expansion at $x$, given $x+h$, $$ f(x+h) \\approx \\sum_{n=0}^\\infty \\frac{h^n}{n!} \\cdot f^{(n)}(x) = f(x) + hf’(x) + \\frac{h^2}{2} f’’(x) + \\dots \\tag*{(1)} $$\nThus, as an approximation for $f’$ using our forward difference approach yields, $$ f’(x) \\approx \\frac{f(x+h) - f(x)}{h} \\approx \\sum_{n=1}^\\infty \\frac{h^{n-1}}{n!} \\cdot f^{(n)}(x) = f’(x) + \\frac{h}{2!} f’’(x) + \\dots $$\nLet $f^\\star$ represent our approximation of $f’$. The truncation error $\\varepsilon_t$ is then given by, $$ \\begin{align} \\varepsilon_t = \\big\\vert f’(x) - f^\\star \\big\\vert = \\Big\\vert \\frac{h}{2!} f’’(x) + \\frac{h^2}{3!} f’’’(x) + \\dots \\Big\\vert = \\mathcal{O}(h f’’) \\end{align} $$\nIf we were to instead look into the central difference method, first take the following Taylor series expansion, $$ f(x-h) \\approx \\sum_{n=0}^\\infty \\frac{(-h)^n}{n!} \\cdot f^{(n)}(x) = f(x) - hf’(x) + \\frac{h^2}{2} f’’(x) - \\dots \\tag*{(2)} $$\nCombining and simplifying equations (1) and (2), $$ \\begin{align} f(x+h) - f(x-h) \u0026\\approx \\sum_{n=0}^\\infty \\frac{h^n}{n!} \\cdot f^{(n)}(x) - \\sum_{n=0}^\\infty \\frac{(-h)^n}{n!} \\cdot f^{(n)}(x)\\\\ \u0026= \\sum_{n=1}^\\infty \\frac{h^n - (-h)^n}{n!} \\cdot f^{(n)}(x)\\\\ \u0026= 2 \\cdot \\sum_{k=1}^\\infty \\frac{h^{2k-1}}{(2k-1)!} \\cdot f^{(2k-1)}(x) \\end{align} $$\nThe central difference approximation then becomes, $$ f’(x) \\approx \\frac{f(x+h) - f(x-h)}{2h} \\approx \\sum_{k=1}^\\infty \\frac{h^{2k-2}}{(2k-1)!} \\cdot f^{(2k-1)}(x) $$\nThe truncation error for this method is noticeably much less than that of the forward difference method. Thus, moving forward throughout this work when taking gradients we will assume the central difference method unless stated otherwise. $$ \\begin{align} \\varepsilon_t = \\big\\vert f’(x) - f^\\star \\big\\vert = \\Big\\vert \\frac{h^2}{3!} f’’’(x) + \\frac{h^4}{5!} f^{(5)}(x) + \\dots \\Big\\vert = \\mathcal{O}(h^2 f’’’) \\end{align} $$\nThis ofcourse leads to the requirement for boundary conditions.\nConsider when $i=0$, our update rule yields $f_0 = \\frac{1}{2h}(f_1 - f_{-1})$. Likewise at $i=n-1$, $f_{n-1} = \\frac{1}{2h}(f_n - f_{n-2})$.\nOne way to get around this is to assume local-continuation at the bounds, $$ f_{-1} = f_0 - (f_1 - f_0) \\qquad \\qquad f_{n} = f_{n-1} + (f_{n-1} - f_{n-2}) $$\nProgrammatically this can be represented by the following, where f denotes the function $f$ and ff its derivative, $f’$.\nx = np.linspace(rmin,rmax,n) y = f(x) yy = ff(x) diff = np.zeros(n) h = (rmax - rmin) / n recip = 1.0 / (2*h) # central difference diff[1:-1] = (y[2:] - y[:-2]) * recip # boundary conditions bot_diff = y[0] - (y[1] - y[0]) diff[0] = (y[1] - bot_diff) * recip top_diff = y[-1] + (y[-1] - y[-2]) diff[-1] = (top_diff - y[-2]) * recip 1.2 Second Order - Single Variable For a second order derivative, there is an altered form for the centred difference, $$ f’(x) \\approx \\frac{f(x+\\frac{1}{2}h) - f(x-\\frac{1}{2}h)}{h} $$\nThis yields, $$ \\begin{align} f’’(x) \u0026\\approx \\frac{f’(x+\\frac{1}{2}h) - f’(x-\\frac{1}{2}h)}{h} \\approx \\frac{1}{h} \\Big( \\frac{f(x+h) - f(x)}{h} - \\frac{f(x) - f(x - h)}{h} \\Big)\\\\ \u0026= \\frac{1}{h^2} \\Big( f(x+h) + f(x - h) - 2\\cdot f(x) \\Big) \\end{align} $$\nProgramatically, this is very similar to the previous example.\nx = np.linspace(rmin,rmax,n) y = f(x) yy = ff(x) diff = np.zeros(n) h = (rmax - rmin) / n recip = 1.0 / (h**2) # central difference diff[1:-1] = (y[2:] + y[:-2] - 2*y[1:-1]) * recip # boundary conditions bot_diff = y[0] - (y[1] - y[0]) diff[0] = (y[1] + bot_diff - 2*y[0]) * recip top_diff = y[-1] + (y[-1] - y[-2]) diff[-1] = (top_diff + y[-2] - 2*y[-1]) * recip 1.3 First Order - Multiple Variables Considering the multivariate case as an extension of the single variable derivation from prior, $$ f_x(x,y) \\approx \\frac{1}{2h} \\big( f(x+h, y) - f(x-h, y) \\big) \\qquad f_y(x,y) \\approx \\frac{1}{2h} \\big( f(x, y+h) - f(x, y-h) \\big) $$\nAs you can expect, this requires boundary conditions for all $(x,y)$ on the boundary of our target region $\\partial R$. In the following implementation, I let the boundary be simply zero. Notice the effect this has in the residual plots,\nh = (rmax - rmin) / n recip = 1.0 / (2*h) X = np.linspace(rmin, rmax, n) Y = np.linspace(rmin, rmax, n) x,y = np.meshgrid(X,Y) z = f(x,y) zx = fx(x,y) zy = fy(x,y) z_bounds = np.zeros([n+2, n+2]) z_bounds[1:-1,1:-1] = z # central differences grad = np.zeros([n, n, 2]) grad[:,:,0] = z_bounds[1:-1, 2:] - z_bounds[1:-1, :-2] # f_x grad[:,:,1] = z_bounds[2:, 1:-1] - z_bounds[:-2, 1:-1] # f_y grad *= recip 1.4 Second Order - Multiple Variables Likewise, the second order extension is, $$ \\begin{align} f_{xx}(x,y) \u0026\\approx \\frac{1}{h^2} \\big( f(x+h, y) + f(x-h, y) - 2f(x,y) \\big)\\\\ f_{yy}(x,y) \u0026\\approx \\frac{1}{h^2} \\big( f(x, y+h) + f(x, y-h) - 2f(x,y) \\big) \\end{align} $$\nz2 = 2 * z_bounds[1:-1, 1:-1] grad = np.zeros([n, n, 2]) grad[:,:,0] = z_bounds[1:-1, 2:] + z_bounds[1:-1, :-2] - z2 # f_xx grad[:,:,1] = z_bounds[2:, 1:-1] + z_bounds[:-2, 1:-1] - z2 # f_yy grad *= recip 2. The Laplace Equation 2.1 Formulation Laplace’s equation is given by the following, $$ \\nabla^2 f = 0 \\qquad \\Leftrightarrow \\qquad \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2} = 0 $$\nFrom the work done in the prior section, we can find an expression for the approximate value of $f$, $$ \\begin{align} 0 \u0026= \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2}\\\\ \u0026\\approx \\frac{1}{h^2}\\Big( f(x+h, y) + f(x-h, y) - 2f(x,y) \\Big) + \\frac{1}{h^2}\\Big( f(x, y+h) + f(x, y-h) - 2f(x,y) \\Big)\\\\ \\end{align} $$\nLeading to, $$ \\begin{align} f(x,y) \u0026\\approx \\frac{1}{4} \\Big( f(x+h, y) + f(x-h, y) + f(x, y+h) + f(x, y-h) \\Big) \\end{align} $$\nNotice $f$ is dependent on its neighbors. If we consider this as an update rule, by iteratively applying the rule the boundary conditions will dissipate outwards and perhaps converge to a solution, where further iterations yield no change.\nLet $f^k_{i,j}$ be the $k$-th iteration, at position $(i,j)$ in the grid. Thus our update rule can be represented by, $$ f^{k+1}_{i,j} = \\frac{1}{4} \\Big( f^k_{i+1,j} + f^k_{i-1,j} + f^k_{i,j+1} + f^k_{i,j-1} \\Big) $$\nWhere $f^0$ is taken to be the initial estimation.\n2.2 Implementation For an example, I considered the heat steady-state interpretation of the Laplace Equation. For each iteration step taken, the output will diffuse the initial heat until eventually it converges to the steady-state.\nAn initial block radiating heat is placed at the center, and kept there through the condition function.\nub = np.zeros([t+1, n+2, n+2]) ub[0] = condition(ub[0], sample) for k in range(t): ub[k+1, 1:-1, 1:-1] = ub[k, 2:, 1:-1] + ub[k, :-2, 1:-1] + \\ ub[k, 1:-1, 2:] + ub[k, 1:-1, :-2] ub[k+1, 1:-1, 1:-1] *= 0.25 # reapply heat source condition ub[k+1] = condition(ub[k+1], sample) Evaluating at n=16 and t=128, showing convergence.\nEvaluating at n=128 and t=128, convergence slows down dramatically.\nEvaluating at n=256 and t=512.\nFrom this we can see that not only is convergence slow, but the rate of convergence is inversely proportional to the dimensions of the initial grid, which makes sense given the propagation process mentioned earlier.\n2.3 Geometric Multi-Gridding Consider the following, where $f^\\star$ denotes the true value, and the error $e$ is given by, $$ f^\\star = f + e \\tag*{(1)} $$\nComputing the error from equation (1) is not as easy as we would like it to be. Thus, instead of finding the error, let the residual $r$ be a measure of the distance between the approximation $\\nabla^2 f$ and the target 0. $$ \\nabla^2 f^\\star = 0 \\qquad \\Rightarrow \\qquad r = 0 - \\nabla^2 f \\tag*{(2)} $$ Computing the residuals can be done by, $$ r_{i,j} = -\\frac{1}{h^2} \\Big( f_{i+1,j} + f_{i-1,j} + f_{i,j+1} + f_{i,j-1} - 4f_{i,j} \\Big) \\tag*{(2b)} $$\nApplying equations (1) and (2) to the original PDE yields an equation for the error. $$ \\nabla^2 f^\\star = 0 \\,\\, \\Leftrightarrow \\,\\, \\nabla^2 (f + e) = 0 \\,\\, \\Leftrightarrow \\,\\, \\nabla^2 e = -\\nabla^2 f \\,\\, \\Leftrightarrow \\,\\, \\nabla^2 e = r \\tag*{(3)} $$\nSolving the error equation can be done in a similar matter to our iterative approach to the Laplace equation, where we have an initial guess for the error, typically zero. Notice how $h$ cancels out from equation (2b). $$ \\begin{align} h^2 r_{i,j} \u0026\\approx e_{i+1,j} + e_{i-1,j} + e_{i,j+1} + e_{i,j-1} - 4e_{i,j}\\\\ \\Rightarrow \\qquad e^{k+1}_{i,j} \u0026= \\frac{1}{4} \\Big( e^k_{i+1,j} + e^k_{i-1,j} + e^k_{i,j+1} + e^k_{i,j-1} - h^2 r_{i,j} \\Big) \\end{align} $$\nOnce we have an approximation for the error, we can use that to correct our approximation of $f$, in a similar notion to that demonstrated in equation (1). $$ \\hat{f}^k_{i,j} = f^k_{i,j} + e^m_{i,j} $$ Where $e^m$ is the error after $m$ iterations, and $\\hat{f}$ represents the corrected approximation.\nAs mentioned prior, the way in which we have formulated our numerical scheme yields equations dependent on the neighbors of each descretized cell. This locally binds our convergence, as the larger the grid size $n$ grows, the longer it will take for information to propagate through the domain.\nGeometric Multi-Gridding attempts to solve the equations recursively over a series of different sized “grids”, the matrices we use. Typically, you will see the largest matrix defined as the fine grid, and the smallest matrix as the coarse grid. The way in which we move through the grids is what defines the type of “cycle”.\nMoving through the grids requires some operations and terminology common to the literature.\nSmoothing / Relaxation In solving the poisson equation $\\nabla^2 f = g$ numerically, the resulting equations have the effect of smoothing the solution. For example, in solving the error equation:\ndef smooth_errors(e,r): # expects bounded matrices x = e[2:, 1:-1] + e[:-2, 1:-1] + \\ e[1:-1, 2:] + e[1:-1, :-2] x -= r[1:-1,1:-1] return x * 0.25 Restriction To move between grids requires a scheme for interpolation. A simple approach to move from a $(2n)^2$ grid to a $n^2$ grid is to take the average of each 2x2 region and collapse that into a single pixel.\ndef restrict(x): y = np.zeros([((i-2)//2) + 2 for i in x.shape]) y[1:-1, 1:-1] = x[1:-1:2, 1:-1:2] + x[2::2, 1:-1:2] + \\ x[1:-1:2, 2::2] + x[2::2, 2::2] return y * 0.25 Prolongation Prolongation is the more complex of the grid operations, requiring the interpolation between points. A surprisingly effective, albeit naive approach, is inverse of the simple restriction. Expand each pixel into a 2x2 region verbatim.\ndef prolong(x): y = np.zeros([(i-1)*2 for i in x.shape]) # expand by factor of 2 y[1:-1:2, 1:-1:2] = x[1:-1, 1:-1] y[2::2, 1:-1:2] = x[1:-1, 1:-1] y[1:-1:2, 2::2] = x[1:-1, 1:-1] y[2::2, 2::2] = x[1:-1, 1:-1] return y Ofcourse, this does not need be so naive.\ndef prolong_smooth(x): y = np.zeros([(i-1)*2 for i in x.shape]) # expand by factor of 2 y[1:-1:2, 1:-1:2] = x[1:-1, 1:-1] y[2::2, 1:-1:2] = x[1:-1, 1:-1] y[1:-1:2, 2::2] = x[1:-1, 1:-1] y[2::2, 2::2] = x[1:-1, 1:-1] # take inner averages y[2:-2, 2:-2] = y[3:-1, 2:-2] + y[1:-3, 2:-2] + \\ y[2:-2, 3:-1] + y[2:-2, 1:-3] y *= 0.25 # compute directional differences at the bounds y2 = 2 * y y[2:-2, 1] = y2[2:-2, 2] - y[2:-2, 3] # left side y[2:-2, -2] = y2[2:-2, -3] - y[2:-2, -4] # right side y[1, 2:-2] = y2[ 2, 2:-2] - y[ 3, 2:-2] # top side y[-2, 2:-2] = y2[-3, 2:-2] - y[-4, 2:-2] # bot side # corner values are the average of their neighbors third = 1.0 / 3.0 y[ 1, 1] = (y[ 1, 2] + y[ 2, 1] + y[ 2, 2]) * third #top left y[ 1,-2] = (y[ 1,-3] + y[ 2,-2] + y[ 2,-3]) * third #top right y[-2, 1] = (y[-2, 2] + y[-3, 1] + y[-3, 2]) * third #bot left y[-2,-2] = (y[-2,-3] + y[-3,-2] + y[-3,-3]) * third #bot right return y V-Cycles A V-Cycle, given in name by the diagram of grid traversals, is algorithmically given by:\nIterate $\\nabla^2 f = 0$ on the coarsest grid. Compute the residuals in the Laplace iterations, $r = - \\nabla^2 f$. Restrict the resisdual to a coarser grid. Iterate $\\nabla^2 e = r$, with initial guess $e = 0$. Compute the residuals in the error, $r = - \\nabla^2 e$. Jump to (3) if not at coarsest grid. Iterate $\\nabla^2 e = r$. Prolong $e$ to a finer grid. Correct the error, $e = e_{fine} + e_{coarse}$. Iterate $\\nabla^2 e = r$. Jump to (8) if not at largest coarse grid. Prolong $e$ to finest grid. Correct the Laplace iteration, $\\hat{f} = f_{} + e$. Graphically this produces the following:\na) Without smooth prolongation:\nb) With smooth prolongation:\nOn the topic of initialisation, a good first estimation would of the problem could ofcourse arrive from merely solving the coarsest problem and prolongating that outwards to the finest grid.\n# solve the coarse problem as an initial guess ub[0] = condition(ub[0],masks[0],r=r) for _ in range(q): ub[0,1:-1,1:-1] = smooth_laplace(ub[0]) ub[0] = condition(ub[0],masks[0],r=r) coarse = restrict(ub[0]) for _ in range(d-1): coarse = restrict(coarse) for _ in range(q): coarse[1:-1,1:-1] = smooth_laplace(coarse) coarse = condition(coarse,masks[-1],r=r) for i in range(d-1): coarse = prolong(coarse) coarse = condition(coarse,masks[-i-2],r=r) ub[0] = prolong(coarse) Convergence can then be specified by a tolerance $\\gamma$, such that $(f^{k+1} - f^{k})^2 \u003c \\gamma$. The implication being that the mean squared error between convergence iterations will decrease until acceptable. Running at a tolerance of $10^{-6}$, for at a size of $256^2$, traversing 6 grids, applying 30 smoothing iterations on each grid, and 50 on the coarsest grid.\nWith initialisation,\nWith smooth prolongation,\nWith initialisation and smooth prolongation,\nW-Cycles Another scheme for grid traversal is the W-Cycle, in which instead of traversing upwards the full path, one:\ntraverses upwards half-way traverse downward to the coarsest grid traverse upwards to the finest grid With initialisation,\nWith smooth prolongation,\nWith initialisation and smooth prolongation,\n2.4 A GPU Coarse-to-Fine Approach Laplace iteration as convolution The laplace iteration can be turned into a kernel used in 2D-convolution, which is very popular in the deep learning field, and has often support for GPU accellerated versions of the operation to allow for massively parallel execution across the grid.\nimport torch.nn.functional as tf ''' 0 | 0.25 | 0 f[i-1,j-1] | f[i,j-1] | f[i+1,j-1] ------------------- ------------------------------------ f[i,j] = 0.25 | 0 | 0.25 * f[i-1,j] | f[i,j] | f[i+1,j] ------------------- ------------------------------------ 0 | 0.25 | 0 f[i-1,j+1] | f[i,j-1] | f[i+1,j+1] ''' w = torch.zeros(1,1,3,3) w[0,0,1,:] = 0.25 w[0,0,:,1] = 0.25 w[0,0,1,1] = 0 multi_grid[i][:,idx:idx+1] = tf.conv2d(multi_grid[i][:,idx:idx+1], w, padding=1) Prolongation as transposed convolution Likewise, the naive prolongation used prior can be done quite easily as a transposed 2D-convolution.\nimport torch.nn.functional as tf ''' f[i,j] | f[i+1,j] 1 | 1 ---------------------- = ----- * f[i,j] f[i,j+1] | f[i+1,j+1] 1 | 1 ''' up = torch.ones(1,1,2,2) multi_grid[i+1][:,0:1,1:-1, 1:-1] = tf.conv_transpose2d(multi_grid[i][:,idx:idx+1,1:-1,1:-1], up, stride=2) This converges much faster than both V-Cycles and W-Cycles. For a $256^2$ fine grid, at 6 traversals and a tolerance of $10^{-6}$.\nExtending to a $2048^2$ fine grid with 10 traversals at a tolerance of $10^{-6}$ is done easily.\n3. [WIP] Poisson and Helmholtz Equations 3.1 [WIP] Poisson’s Equation Naive $$ \\begin{aligned} g \u0026= \\nabla^2 f\\\\ g_{i,j} \u0026= \\frac{1}{h^2} \\Big( f_{i+1,j} + f_{i-1,j} + f_{i,j+1} + f_{i,j-1} - 4f_{i,j} \\Big)\\\\ f_{i,j} \u0026= \\frac{1}{4} \\Big( f_{i+1,j} + f_{i-1,j} + f_{i,j+1} + f_{i,j-1} - h^2g_{i,j} \\Big) \\end{aligned} $$\nMultigridding $$ f^\\star = f + e \\tag*{(1)} $$\n$$ \\nabla^2 f^\\star = g \\qquad \\Rightarrow \\qquad r = g - \\nabla^2 f \\tag*{(2)} $$\n$$ h^2 r_{i,j} = h^2 g_{i,j} - \\Big( f_{i+1,j} + f_{i-1,j} + f_{i,j+1} + f_{i,j-1} - 4f_{i,j} \\Big) $$\n$$ \\nabla^2 f^\\star = 0 ,, \\Leftrightarrow ,, \\nabla^2 (f + e) = 0 ,, \\Leftrightarrow ,, \\nabla^2 e = -\\nabla^2 f \\qquad \\Leftrightarrow \\qquad \\nabla^2 e = r \\tag*{(3)} $$\n$$ \\begin{aligned} h^2 r_{i,j} \u0026\\approx e_{i+1,j} + e_{i-1,j} + e_{i,j+1} + e_{i,j-1} - 4e_{i,j}\\\\ \\Rightarrow \\qquad e^{k+1}_{i,j} \u0026= \\frac{1}{4} \\Big( e^k_{i+1,j} + e^k_{i-1,j} + e^k_{i,j+1} + e^k_{i,j-1} - h^2 r_{i,j} \\Big) \\end{aligned} $$\n3.2 [x] Time-Dependency in the Forcing Term 3.3 [x] Helmholtz Equation 3.4 [x] Generalized Form with Time Dependence 4. [x] The Advection-Diffusion Equation 4.1 [x] One Spatial Dimension 4.2 [x] Two Spatial Dimensions 5. [x] Rayleigh-Benard Convection ","wordCount":"3000","inLanguage":"en","datePublished":"2020-11-15T00:00:00Z","dateModified":"2020-11-15T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"/post/finite_differences/"},"publisher":{"@type":"Organization","name":"Russel Demos blog","logo":{"@type":"ImageObject","url":"%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href accesskey=h title="Russel Demos blog (Alt + H)">Russel Demos blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=/categories/ title=categories><span>categories</span></a></li><li><a href=/tags/ title=tags><span>tags</span></a></li><li><a href=https://veemon.github.io title=links><span>links</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href>Home</a>&nbsp;»&nbsp;<a href=/post/>Posts</a></div><h1 class=post-title>Finite_differences</h1><div class=post-description>... Description ...</div><div class=post-meta><span title='2020-11-15 00:00:00 +0000 UTC'>November 15, 2020</span>&nbsp;·&nbsp;15 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#motivation>Motivation</a></li><li><a href=#1-recap-of-numerical-differentiation>1. Recap of Numerical Differentiation</a><ul><li><a href=#11-first-order---single-variable>1.1 First Order - Single Variable</a></li><li><a href=#12-second-order---single-variable>1.2 Second Order - Single Variable</a></li><li><a href=#13-first-order---multiple-variables>1.3 First Order - Multiple Variables</a></li><li><a href=#14-second-order---multiple-variables>1.4 Second Order - Multiple Variables</a></li></ul></li><li><a href=#2-the-laplace-equation>2. The Laplace Equation</a><ul><li><a href=#21-formulation>2.1 Formulation</a></li><li><a href=#22-implementation>2.2 Implementation</a></li><li><a href=#23-geometric-multi-gridding>2.3 Geometric Multi-Gridding</a></li><li><a href=#24-a-gpu-coarse-to-fine-approach>2.4 A GPU Coarse-to-Fine Approach</a></li></ul></li><li><a href=#3-wip-poisson-and-helmholtz-equations>3. [WIP] Poisson and Helmholtz Equations</a><ul><li><a href=#31-wip-poissons-equation>3.1 [WIP] Poisson&rsquo;s Equation</a></li><li><a href=#32-x-time-dependency-in-the-forcing-term>3.2 [x] Time-Dependency in the Forcing Term</a></li><li><a href=#33-x-helmholtz-equation>3.3 [x] Helmholtz Equation</a></li><li><a href=#34-x-generalized-form-with-time-dependence>3.4 [x] Generalized Form with Time Dependence</a></li></ul></li><li><a href=#4-x-the-advection-diffusion-equation>4. [x] The Advection-Diffusion Equation</a><ul><li><a href=#41-x-one-spatial-dimension>4.1 [x] One Spatial Dimension</a></li><li><a href=#42-x-two-spatial-dimensions>4.2 [x] Two Spatial Dimensions</a></li></ul></li><li><a href=#5-x-rayleigh-benard-convection>5. [x] Rayleigh-Benard Convection</a></li></ul></nav></div></details></div><div class=post-content><div class=highlight><pre tabindex=0 class=chroma><code class=language-md data-lang=md><span class=line><span class=cl>For some context behind this post, I had just completed my first year in mathematics
</span></span><span class=line><span class=cl>and was inspired by the PDE&#39;s part of a differential equations class I took. 
</span></span><span class=line><span class=cl>At this point, I had not taken analysis however I had read Spivak&#39;s book the year
</span></span><span class=line><span class=cl>prior. Nonetheless, I was <span class=ge>_excited_</span> to be doing math after a terrible first year,
</span></span><span class=line><span class=cl>and I still do think about &#34;simulation for artistic recreation&#34;, 
</span></span><span class=line><span class=cl>as I will say in the upcoming motivation. It&#39;s a bit crazy to think how much I would
</span></span><span class=line><span class=cl>encounter and learn in the years following this post ...
</span></span><span class=line><span class=cl>                                                            <span class=k>-</span> Russel, June 28, 2023.
</span></span></code></pre></div><h1 id=can-we-apply-deep-learning-to-pdes>Can we apply deep learning to PDE&rsquo;s?<a hidden class=anchor aria-hidden=true href=#can-we-apply-deep-learning-to-pdes>#</a></h1><h2 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h2><p>I am interested in large-scale simulation for artistic recreation.
I find a lot of the patterns and chaotic dynamics found in the natural world
quite beautiful, as I&rsquo;m sure most people do. In a sort of &ldquo;aesthetically&rdquo;-driven
pursuit of replicating this, I hope I can learn more about a wide array
of fields and potentially produce novel results.</p><p>Although in general I wish to make use of geo-physical simulation
for planet-scale landform generation, in this work I am particularly
building up to modelling mantle convection and the interactions between
the mantle and the lithosphere. With such results, as I wish to create
a pseudo-real-time simulation, I will look into approximation methods that
utilize modern techniques from deep learning.</p><p><a href=https://github.com/Veemon/pde><img loading=lazy src=/github_black.svg alt=repository_link></a></p><h2 id=1-recap-of-numerical-differentiation>1. Recap of Numerical Differentiation<a hidden class=anchor aria-hidden=true href=#1-recap-of-numerical-differentiation>#</a></h2><h3 id=11-first-order---single-variable>1.1 First Order - Single Variable<a hidden class=anchor aria-hidden=true href=#11-first-order---single-variable>#</a></h3><p>There are many ways to calculate the slope of the secant line at a point $x$.
$$
f&rsquo;(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h} \tag*{(Forward Difference)}
$$</p><p>$$
f&rsquo;(x) = \lim_{h \to 0} \frac{f(x) - f(x-h)}{h} \tag*{(Backward Difference)}
$$</p><p>$$
f&rsquo;(x) = \lim_{h \to 0} \frac{f(x+h) - f(x-h)}{2h} \tag*{(Centered Difference)}
$$</p><p>Ofcourse, for the numerical approach instead of a continuum of values
for a function $f$, we have the descretization $f_i$ over the ordered set of
sample points $i \in N$. Each $f_i$ is thus sampled a distance of
$\Delta x = \frac{1}{n} (R_{max} - R_{min})$ away from the last point,
over a domain region $[R_{min}, R_{max}]$.</p><p>Recall the Taylor Series for a function $f$, evaluated at some $a$
for a given $x_0$,
$$
f(x_0) \approx \sum_{n=0}^\infty \frac{(x_0 - a)^n}{n!} \cdot f^{(n)}(a)
$$</p><p>If we were to take the forward difference approach, consider
taking the Taylor series expansion at $x$, given $x+h$,
$$
f(x+h) \approx \sum_{n=0}^\infty \frac{h^n}{n!} \cdot f^{(n)}(x)
= f(x) + hf&rsquo;(x) + \frac{h^2}{2} f&rsquo;&rsquo;(x) + \dots \tag*{(1)}
$$</p><p>Thus, as an approximation for $f&rsquo;$ using our forward difference approach yields,
$$
f&rsquo;(x) \approx \frac{f(x+h) - f(x)}{h}
\approx \sum_{n=1}^\infty \frac{h^{n-1}}{n!} \cdot f^{(n)}(x)
= f&rsquo;(x) + \frac{h}{2!} f&rsquo;&rsquo;(x) + \dots
$$</p><p>Let $f^\star$ represent our approximation of $f&rsquo;$.
The truncation error $\varepsilon_t$ is then given by,
$$
\begin{align}
\varepsilon_t
= \big\vert f&rsquo;(x) - f^\star \big\vert
= \Big\vert \frac{h}{2!} f&rsquo;&rsquo;(x) + \frac{h^2}{3!} f&rsquo;&rsquo;&rsquo;(x) + \dots \Big\vert
= \mathcal{O}(h f&rsquo;&rsquo;)
\end{align}
$$</p><p>If we were to instead look into the central difference method,
first take the following Taylor series expansion,
$$
f(x-h) \approx \sum_{n=0}^\infty \frac{(-h)^n}{n!} \cdot f^{(n)}(x)
= f(x) - hf&rsquo;(x) + \frac{h^2}{2} f&rsquo;&rsquo;(x) - \dots \tag*{(2)}
$$</p><p>Combining and simplifying equations (1) and (2),
$$
\begin{align}
f(x+h) - f(x-h) &\approx \sum_{n=0}^\infty \frac{h^n}{n!} \cdot f^{(n)}(x) - \sum_{n=0}^\infty \frac{(-h)^n}{n!} \cdot f^{(n)}(x)\\
&= \sum_{n=1}^\infty \frac{h^n - (-h)^n}{n!} \cdot f^{(n)}(x)\\
&= 2 \cdot \sum_{k=1}^\infty \frac{h^{2k-1}}{(2k-1)!} \cdot f^{(2k-1)}(x)
\end{align}
$$</p><p>The central difference approximation then becomes,
$$
f&rsquo;(x) \approx \frac{f(x+h) - f(x-h)}{2h}
\approx \sum_{k=1}^\infty \frac{h^{2k-2}}{(2k-1)!} \cdot f^{(2k-1)}(x)
$$</p><p>The truncation error for this method is noticeably <strong>much less</strong> than that
of the forward difference method. Thus, moving forward throughout this
work when taking gradients we will assume the central difference method
unless stated otherwise.
$$
\begin{align}
\varepsilon_t
= \big\vert f&rsquo;(x) - f^\star \big\vert
= \Big\vert \frac{h^2}{3!} f&rsquo;&rsquo;&rsquo;(x) + \frac{h^4}{5!} f^{(5)}(x) + \dots \Big\vert
= \mathcal{O}(h^2 f&rsquo;&rsquo;&rsquo;)
\end{align}
$$</p><p>This ofcourse leads to the requirement for <em>boundary conditions</em>.<br>Consider when $i=0$, our update rule yields
$f_0 = \frac{1}{2h}(f_1 - f_{-1})$.
Likewise at $i=n-1$,
$f_{n-1} = \frac{1}{2h}(f_n - f_{n-2})$.</p><p>One way to get around this is to assume local-continuation at the bounds,
$$
f_{-1} = f_0 - (f_1 - f_0) \qquad \qquad
f_{n} = f_{n-1} + (f_{n-1} - f_{n-2})
$$</p><p>Programmatically this can be represented by the following, where <code>f</code>
denotes the function $f$ and <code>ff</code> its derivative, $f&rsquo;$.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span>  <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>rmin</span><span class=p>,</span><span class=n>rmax</span><span class=p>,</span><span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span>  <span class=o>=</span> <span class=n>f</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>yy</span> <span class=o>=</span> <span class=n>ff</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>diff</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>h</span> <span class=o>=</span> <span class=p>(</span><span class=n>rmax</span> <span class=o>-</span> <span class=n>rmin</span><span class=p>)</span> <span class=o>/</span> <span class=n>n</span>
</span></span><span class=line><span class=cl><span class=n>recip</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=mi>2</span><span class=o>*</span><span class=n>h</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># central difference</span>
</span></span><span class=line><span class=cl><span class=n>diff</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=n>y</span><span class=p>[</span><span class=mi>2</span><span class=p>:]</span> <span class=o>-</span> <span class=n>y</span><span class=p>[:</span><span class=o>-</span><span class=mi>2</span><span class=p>])</span> <span class=o>*</span> <span class=n>recip</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># boundary conditions</span>
</span></span><span class=line><span class=cl><span class=n>bot_diff</span> <span class=o>=</span> <span class=n>y</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>-</span> <span class=p>(</span><span class=n>y</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=n>y</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>diff</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=n>y</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=n>bot_diff</span><span class=p>)</span> <span class=o>*</span> <span class=n>recip</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>top_diff</span> <span class=o>=</span> <span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=p>(</span><span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>diff</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=n>top_diff</span> <span class=o>-</span> <span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>])</span> <span class=o>*</span> <span class=n>recip</span>
</span></span></code></pre></div><p><img loading=lazy src=/finite_differences/1_1_sin.png alt></p><h3 id=12-second-order---single-variable>1.2 Second Order - Single Variable<a hidden class=anchor aria-hidden=true href=#12-second-order---single-variable>#</a></h3><p>For a second order derivative, there is an altered form for the centred
difference,
$$
f&rsquo;(x) \approx \frac{f(x+\frac{1}{2}h) - f(x-\frac{1}{2}h)}{h}
$$</p><p>This yields,
$$
\begin{align}
f&rsquo;&rsquo;(x)
&\approx \frac{f&rsquo;(x+\frac{1}{2}h) - f&rsquo;(x-\frac{1}{2}h)}{h}
\approx \frac{1}{h} \Big( \frac{f(x+h) - f(x)}{h} - \frac{f(x) - f(x - h)}{h} \Big)\\
&= \frac{1}{h^2} \Big( f(x+h) + f(x - h) - 2\cdot f(x) \Big)
\end{align}
$$</p><p>Programatically, this is very similar to the previous example.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span>  <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>rmin</span><span class=p>,</span><span class=n>rmax</span><span class=p>,</span><span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span>  <span class=o>=</span> <span class=n>f</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>yy</span> <span class=o>=</span> <span class=n>ff</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>diff</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>h</span> <span class=o>=</span> <span class=p>(</span><span class=n>rmax</span> <span class=o>-</span> <span class=n>rmin</span><span class=p>)</span> <span class=o>/</span> <span class=n>n</span>
</span></span><span class=line><span class=cl><span class=n>recip</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=n>h</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># central difference</span>
</span></span><span class=line><span class=cl><span class=n>diff</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=n>y</span><span class=p>[</span><span class=mi>2</span><span class=p>:]</span> <span class=o>+</span> <span class=n>y</span><span class=p>[:</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=o>-</span> <span class=mi>2</span><span class=o>*</span><span class=n>y</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span> <span class=o>*</span> <span class=n>recip</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># boundary conditions</span>
</span></span><span class=line><span class=cl><span class=n>bot_diff</span> <span class=o>=</span> <span class=n>y</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>-</span> <span class=p>(</span><span class=n>y</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=n>y</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>diff</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=n>y</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>bot_diff</span> <span class=o>-</span> <span class=mi>2</span><span class=o>*</span><span class=n>y</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span> <span class=o>*</span> <span class=n>recip</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>top_diff</span> <span class=o>=</span> <span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=p>(</span><span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>diff</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=n>top_diff</span> <span class=o>+</span> <span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=o>-</span> <span class=mi>2</span><span class=o>*</span><span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span> <span class=o>*</span> <span class=n>recip</span>
</span></span></code></pre></div><p><img loading=lazy src=/finite_differences/1_2_sin.png alt></p><h3 id=13-first-order---multiple-variables>1.3 First Order - Multiple Variables<a hidden class=anchor aria-hidden=true href=#13-first-order---multiple-variables>#</a></h3><p>Considering the multivariate case as an extension of the single variable derivation
from prior,
$$
f_x(x,y) \approx \frac{1}{2h} \big( f(x+h, y) - f(x-h, y) \big) \qquad
f_y(x,y) \approx \frac{1}{2h} \big( f(x, y+h) - f(x, y-h) \big)
$$</p><p>As you can expect, this requires boundary conditions for all $(x,y)$ on
the boundary of our target region $\partial R$.
In the following implementation, I let the boundary be simply zero.
Notice the effect this has in the residual plots,</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>h</span> <span class=o>=</span> <span class=p>(</span><span class=n>rmax</span> <span class=o>-</span> <span class=n>rmin</span><span class=p>)</span> <span class=o>/</span> <span class=n>n</span>
</span></span><span class=line><span class=cl><span class=n>recip</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=mi>2</span><span class=o>*</span><span class=n>h</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>rmin</span><span class=p>,</span> <span class=n>rmax</span><span class=p>,</span> <span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>Y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>rmin</span><span class=p>,</span> <span class=n>rmax</span><span class=p>,</span> <span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x</span><span class=p>,</span><span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>meshgrid</span><span class=p>(</span><span class=n>X</span><span class=p>,</span><span class=n>Y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>z</span>  <span class=o>=</span> <span class=n>f</span><span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>zx</span> <span class=o>=</span> <span class=n>fx</span><span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>zy</span> <span class=o>=</span> <span class=n>fy</span><span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>z_bounds</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>([</span><span class=n>n</span><span class=o>+</span><span class=mi>2</span><span class=p>,</span> <span class=n>n</span><span class=o>+</span><span class=mi>2</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>z_bounds</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>z</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># central differences</span>
</span></span><span class=line><span class=cl><span class=n>grad</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>([</span><span class=n>n</span><span class=p>,</span> <span class=n>n</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>grad</span><span class=p>[:,:,</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>z_bounds</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>:]</span> <span class=o>-</span> <span class=n>z_bounds</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span>  <span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span>  <span class=c1># f_x</span>
</span></span><span class=line><span class=cl><span class=n>grad</span><span class=p>[:,:,</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>z_bounds</span><span class=p>[</span><span class=mi>2</span><span class=p>:,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=n>z_bounds</span><span class=p>[:</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span>  <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>  <span class=c1># f_y</span>
</span></span><span class=line><span class=cl><span class=n>grad</span> <span class=o>*=</span> <span class=n>recip</span>
</span></span></code></pre></div><p><img loading=lazy src=/finite_differences/1_3_sin.png alt></p><h3 id=14-second-order---multiple-variables>1.4 Second Order - Multiple Variables<a hidden class=anchor aria-hidden=true href=#14-second-order---multiple-variables>#</a></h3><p>Likewise, the second order extension is,
$$
\begin{align}
f_{xx}(x,y) &\approx \frac{1}{h^2} \big( f(x+h, y) + f(x-h, y) - 2f(x,y) \big)\\
f_{yy}(x,y) &\approx \frac{1}{h^2} \big( f(x, y+h) + f(x, y-h) - 2f(x,y) \big)
\end{align}
$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>z2</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>z_bounds</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>grad</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>([</span><span class=n>n</span><span class=p>,</span> <span class=n>n</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>grad</span><span class=p>[:,:,</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>z_bounds</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>:]</span> <span class=o>+</span> <span class=n>z_bounds</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span>  <span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=o>-</span> <span class=n>z2</span>  <span class=c1># f_xx</span>
</span></span><span class=line><span class=cl><span class=n>grad</span><span class=p>[:,:,</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>z_bounds</span><span class=p>[</span><span class=mi>2</span><span class=p>:,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>z_bounds</span><span class=p>[:</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span>  <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=n>z2</span>  <span class=c1># f_yy</span>
</span></span><span class=line><span class=cl><span class=n>grad</span> <span class=o>*=</span> <span class=n>recip</span>
</span></span></code></pre></div><p><img loading=lazy src=/finite_differences/1_4_sin.png alt></p><h2 id=2-the-laplace-equation>2. The Laplace Equation<a hidden class=anchor aria-hidden=true href=#2-the-laplace-equation>#</a></h2><h3 id=21-formulation>2.1 Formulation<a hidden class=anchor aria-hidden=true href=#21-formulation>#</a></h3><p>Laplace&rsquo;s equation is given by the following,
$$
\nabla^2 f = 0 \qquad \Leftrightarrow \qquad
\frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2} = 0
$$</p><p>From the work done in the prior section, we can find an expression
for the approximate value of $f$,
$$
\begin{align}
0 &= \frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2}\\
&\approx \frac{1}{h^2}\Big( f(x+h, y) + f(x-h, y) - 2f(x,y) \Big)
+ \frac{1}{h^2}\Big( f(x, y+h) + f(x, y-h) - 2f(x,y) \Big)\\
\end{align}
$$</p><p>Leading to,
$$
\begin{align}
f(x,y) &\approx \frac{1}{4} \Big( f(x+h, y) + f(x-h, y) + f(x, y+h) + f(x, y-h) \Big)
\end{align}
$$</p><p>Notice $f$ is dependent on its neighbors.
If we consider this as an update rule,
by iteratively applying the rule the boundary conditions will
dissipate outwards and perhaps converge to a solution,
where further iterations yield no change.</p><p>Let $f^k_{i,j}$ be the $k$-th iteration, at position $(i,j)$ in the grid. Thus our update rule can be represented by,
$$
f^{k+1}_{i,j} =
\frac{1}{4} \Big( f^k_{i+1,j} + f^k_{i-1,j} + f^k_{i,j+1} + f^k_{i,j-1} \Big)
$$</p><p>Where $f^0$ is taken to be the initial estimation.</p><h3 id=22-implementation>2.2 Implementation<a hidden class=anchor aria-hidden=true href=#22-implementation>#</a></h3><p>For an example, I considered the heat steady-state
interpretation of the Laplace Equation. For each iteration step
taken, the output will diffuse the initial heat until eventually
it converges to the steady-state.</p><p>An initial block radiating heat is placed at the center, and kept there through the <code>condition</code> function.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>ub</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>([</span><span class=n>t</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span> <span class=n>n</span><span class=o>+</span><span class=mi>2</span><span class=p>,</span> <span class=n>n</span><span class=o>+</span><span class=mi>2</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>ub</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>condition</span><span class=p>(</span><span class=n>ub</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>sample</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>t</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>ub</span><span class=p>[</span><span class=n>k</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>ub</span><span class=p>[</span><span class=n>k</span><span class=p>,</span> <span class=mi>2</span><span class=p>:,</span>  <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>ub</span><span class=p>[</span><span class=n>k</span><span class=p>,</span> <span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> \
</span></span><span class=line><span class=cl>                          <span class=n>ub</span><span class=p>[</span><span class=n>k</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>:]</span>  <span class=o>+</span> <span class=n>ub</span><span class=p>[</span><span class=n>k</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>ub</span><span class=p>[</span><span class=n>k</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>*=</span> <span class=mf>0.25</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># reapply heat source condition</span>
</span></span><span class=line><span class=cl>    <span class=n>ub</span><span class=p>[</span><span class=n>k</span><span class=o>+</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>condition</span><span class=p>(</span><span class=n>ub</span><span class=p>[</span><span class=n>k</span><span class=o>+</span><span class=mi>1</span><span class=p>],</span> <span class=n>sample</span><span class=p>)</span>
</span></span></code></pre></div><p>Evaluating at <code>n=16</code> and <code>t=128</code>, showing convergence.<br><img loading=lazy src=/finite_differences/2_2_laplace_16_128.gif alt></p><p>Evaluating at <code>n=128</code> and <code>t=128</code>, convergence slows down dramatically.<br><img loading=lazy src=/finite_differences/2_2_laplace_128_128.gif alt></p><p>Evaluating at <code>n=256</code> and <code>t=512</code>.<br><img loading=lazy src=/finite_differences/2_2_laplace_256_512.gif alt></p><p>From this we can see that not only is convergence slow, but the rate of
convergence is inversely proportional to the dimensions of the initial grid,
which makes sense given the propagation process mentioned earlier.</p><h3 id=23-geometric-multi-gridding>2.3 Geometric Multi-Gridding<a hidden class=anchor aria-hidden=true href=#23-geometric-multi-gridding>#</a></h3><p>Consider the following, where $f^\star$ denotes the true value, and the error $e$ is given by,
$$
f^\star = f + e \tag*{(1)}
$$</p><p>Computing the error from equation (1) is not as easy as we would like it to be.
Thus, instead of finding the error, let the residual $r$ be a measure
of the distance between the approximation $\nabla^2 f$ and the target 0.
$$
\nabla^2 f^\star = 0 \qquad \Rightarrow \qquad
r = 0 - \nabla^2 f \tag*{(2)}
$$
Computing the residuals can be done by,
$$
r_{i,j} = -\frac{1}{h^2} \Big(
f_{i+1,j} + f_{i-1,j} + f_{i,j+1} + f_{i,j-1} - 4f_{i,j}
\Big) \tag*{(2b)}
$$</p><p>Applying equations (1) and (2) to the original PDE yields
an equation for the error.
$$
\nabla^2 f^\star = 0 \,\, \Leftrightarrow \,\,
\nabla^2 (f + e) = 0 \,\, \Leftrightarrow \,\,
\nabla^2 e = -\nabla^2 f \,\, \Leftrightarrow \,\,
\nabla^2 e = r \tag*{(3)}
$$</p><p>Solving the error equation can be done in a similar matter to our
iterative approach to the Laplace equation, where we have an initial
guess for the error, typically zero.
<strong>Notice</strong> how $h$ <strong>cancels out</strong> from equation (2b).
$$
\begin{align}
h^2 r_{i,j} &\approx e_{i+1,j} + e_{i-1,j} + e_{i,j+1} + e_{i,j-1} - 4e_{i,j}\\
\Rightarrow \qquad e^{k+1}_{i,j} &=
\frac{1}{4} \Big(
e^k_{i+1,j} + e^k_{i-1,j} + e^k_{i,j+1} + e^k_{i,j-1} - h^2 r_{i,j} \Big)
\end{align}
$$</p><p>Once we have an approximation for the error, we can use that to correct
our approximation of $f$, in a similar notion to that demonstrated in equation (1).
$$
\hat{f}^k_{i,j} = f^k_{i,j} + e^m_{i,j}
$$
Where $e^m$ is the error after $m$ iterations, and $\hat{f}$ represents the
corrected approximation.</p><p>As mentioned prior, the way in which we have formulated our numerical scheme
yields equations dependent on the neighbors of each descretized cell.
This locally binds our convergence, as the larger the grid size $n$ grows,
the longer it will take for information to propagate through the domain.</p><p>Geometric Multi-Gridding attempts to solve the equations recursively
over a series of different sized &ldquo;grids&rdquo;, the matrices we use.
Typically, you will see the largest matrix defined as the <strong>fine</strong> grid,
and the smallest matrix as the <strong>coarse</strong> grid. The way in which
we move through the grids is what defines the type of &ldquo;cycle&rdquo;.</p><p>Moving through the grids requires some operations and terminology common to the literature.</p><h4 id=smoothing--relaxation>Smoothing / Relaxation<a hidden class=anchor aria-hidden=true href=#smoothing--relaxation>#</a></h4><p><em>In solving the poisson equation $\nabla^2 f = g$ numerically, the resulting equations have
the effect of smoothing the solution. For example, in solving the error equation:</em></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>smooth_errors</span><span class=p>(</span><span class=n>e</span><span class=p>,</span><span class=n>r</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># expects bounded matrices</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>e</span><span class=p>[</span><span class=mi>2</span><span class=p>:,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>e</span><span class=p>[:</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> \
</span></span><span class=line><span class=cl>        <span class=n>e</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>:]</span> <span class=o>+</span> <span class=n>e</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>-=</span> <span class=n>r</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>x</span> <span class=o>*</span> <span class=mf>0.25</span>
</span></span></code></pre></div><h4 id=restriction>Restriction<a hidden class=anchor aria-hidden=true href=#restriction>#</a></h4><p><em>To move between grids requires a scheme for interpolation. A simple approach to move from a $(2n)^2$
grid to a $n^2$ grid is to take the average of each 2x2 region and collapse that into a single pixel.</em></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>restrict</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>([((</span><span class=n>i</span><span class=o>-</span><span class=mi>2</span><span class=p>)</span><span class=o>//</span><span class=mi>2</span><span class=p>)</span> <span class=o>+</span> <span class=mi>2</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>:</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>:</span><span class=mi>2</span><span class=p>]</span> <span class=o>+</span> <span class=n>x</span><span class=p>[</span><span class=mi>2</span><span class=p>::</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>:</span><span class=mi>2</span><span class=p>]</span> <span class=o>+</span> \
</span></span><span class=line><span class=cl>                    <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>:</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span>   <span class=o>+</span> <span class=n>x</span><span class=p>[</span><span class=mi>2</span><span class=p>::</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>y</span> <span class=o>*</span> <span class=mf>0.25</span>
</span></span></code></pre></div><h4 id=prolongation>Prolongation<a hidden class=anchor aria-hidden=true href=#prolongation>#</a></h4><p><em>Prolongation is the more complex of the grid operations, requiring the interpolation between points.
A surprisingly effective, albeit naive approach, is inverse of the simple restriction. Expand
each pixel into a 2x2 region verbatim.</em></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>prolong</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>([(</span><span class=n>i</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span><span class=o>*</span><span class=mi>2</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># expand by factor of 2</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>:</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>:</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=p>[</span><span class=mi>2</span><span class=p>::</span><span class=mi>2</span><span class=p>,</span>   <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>:</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>:</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span>   <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=p>[</span><span class=mi>2</span><span class=p>::</span><span class=mi>2</span><span class=p>,</span>   <span class=mi>2</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span>   <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>y</span>
</span></span></code></pre></div><p>Ofcourse, this does not need be so naive.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>prolong_smooth</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>([(</span><span class=n>i</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span><span class=o>*</span><span class=mi>2</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># expand by factor of 2</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>:</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>:</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=p>[</span><span class=mi>2</span><span class=p>::</span><span class=mi>2</span><span class=p>,</span>   <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>:</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>:</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span>   <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=p>[</span><span class=mi>2</span><span class=p>::</span><span class=mi>2</span><span class=p>,</span>   <span class=mi>2</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span>   <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># take inner averages</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=p>[</span><span class=mi>2</span><span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>y</span><span class=p>[</span><span class=mi>3</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=o>+</span> <span class=n>y</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=o>+</span> \
</span></span><span class=line><span class=cl>                    <span class=n>y</span><span class=p>[</span><span class=mi>2</span><span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>y</span><span class=p>[</span><span class=mi>2</span><span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>3</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>*=</span> <span class=mf>0.25</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># compute directional differences at the bounds</span>
</span></span><span class=line><span class=cl>    <span class=n>y2</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=p>[</span><span class=mi>2</span><span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span>  <span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>y2</span><span class=p>[</span><span class=mi>2</span><span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span>  <span class=mi>2</span><span class=p>]</span> <span class=o>-</span> <span class=n>y</span><span class=p>[</span><span class=mi>2</span><span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span>  <span class=mi>3</span><span class=p>]</span> <span class=c1># left  side</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=p>[</span><span class=mi>2</span><span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>y2</span><span class=p>[</span><span class=mi>2</span><span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>3</span><span class=p>]</span> <span class=o>-</span> <span class=n>y</span><span class=p>[</span><span class=mi>2</span><span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>4</span><span class=p>]</span> <span class=c1># right side</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span>  <span class=mi>2</span><span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>y2</span><span class=p>[</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=o>-</span> <span class=n>y</span><span class=p>[</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=c1># top   side</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>y2</span><span class=p>[</span><span class=o>-</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=o>-</span> <span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>4</span><span class=p>,</span> <span class=mi>2</span><span class=p>:</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=c1># bot   side</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># corner values are the average of their neighbors</span>
</span></span><span class=line><span class=cl>    <span class=n>third</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=mf>3.0</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=p>[</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=n>y</span><span class=p>[</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>]</span> <span class=o>+</span> <span class=n>y</span><span class=p>[</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>y</span><span class=p>[</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span> <span class=o>*</span> <span class=n>third</span> <span class=c1>#top left</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=p>[</span> <span class=mi>1</span><span class=p>,</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=n>y</span><span class=p>[</span> <span class=mi>1</span><span class=p>,</span><span class=o>-</span><span class=mi>3</span><span class=p>]</span> <span class=o>+</span> <span class=n>y</span><span class=p>[</span> <span class=mi>2</span><span class=p>,</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=o>+</span> <span class=n>y</span><span class=p>[</span> <span class=mi>2</span><span class=p>,</span><span class=o>-</span><span class=mi>3</span><span class=p>])</span> <span class=o>*</span> <span class=n>third</span> <span class=c1>#top right</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>]</span> <span class=o>+</span> <span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span> <span class=o>*</span> <span class=n>third</span> <span class=c1>#bot left</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span><span class=o>-</span><span class=mi>3</span><span class=p>]</span> <span class=o>+</span> <span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>3</span><span class=p>,</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=o>+</span> <span class=n>y</span><span class=p>[</span><span class=o>-</span><span class=mi>3</span><span class=p>,</span><span class=o>-</span><span class=mi>3</span><span class=p>])</span> <span class=o>*</span> <span class=n>third</span> <span class=c1>#bot right</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>y</span>
</span></span></code></pre></div><h4 id=v-cycles>V-Cycles<a hidden class=anchor aria-hidden=true href=#v-cycles>#</a></h4><p>A V-Cycle, given in name by the diagram of grid traversals,
is algorithmically given by:</p><ul><li><ol><li>Iterate $\nabla^2 f = 0$ on the coarsest grid.</li></ol></li><li><ol start=2><li>Compute the residuals in the Laplace iterations, $r = - \nabla^2 f$.</li></ol></li><li><ol start=3><li>Restrict the resisdual to a coarser grid.</li></ol></li><li><ol start=4><li>Iterate $\nabla^2 e = r$, with initial guess $e = 0$.</li></ol></li><li><ol start=5><li>Compute the residuals in the error, $r = - \nabla^2 e$.</li></ol></li><li><ol start=6><li>Jump to (3) if not at coarsest grid.</li></ol></li><li><ol start=7><li>Iterate $\nabla^2 e = r$.</li></ol></li><li><ol start=8><li>Prolong $e$ to a finer grid.</li></ol></li><li><ol start=9><li>Correct the error, $e = e_{fine} + e_{coarse}$.</li></ol></li><li><ol start=10><li>Iterate $\nabla^2 e = r$.</li></ol></li><li><ol start=11><li>Jump to (8) if not at largest coarse grid.</li></ol></li><li><ol start=12><li>Prolong $e$ to finest grid.</li></ol></li><li><ol start=13><li>Correct the Laplace iteration, $\hat{f} = f_{} + e$.</li></ol></li></ul><p>Graphically this produces the following:</p><ul><li>a) Without smooth prolongation:<br><img loading=lazy src=/finite_differences/2_3_diagram_init.png alt></li><li>b) With smooth prolongation:<br><img loading=lazy src=/finite_differences/2_3_diagram_init_smooth.png alt></li></ul><p>On the topic of initialisation, a good first estimation would of the problem
could ofcourse arrive from merely solving the coarsest problem and prolongating
that outwards to the finest grid.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># solve the coarse problem as an initial guess</span>
</span></span><span class=line><span class=cl><span class=n>ub</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>condition</span><span class=p>(</span><span class=n>ub</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span><span class=n>masks</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span><span class=n>r</span><span class=o>=</span><span class=n>r</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>q</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>ub</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>smooth_laplace</span><span class=p>(</span><span class=n>ub</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>ub</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>condition</span><span class=p>(</span><span class=n>ub</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span><span class=n>masks</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span><span class=n>r</span><span class=o>=</span><span class=n>r</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>coarse</span> <span class=o>=</span> <span class=n>restrict</span><span class=p>(</span><span class=n>ub</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>d</span><span class=o>-</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>coarse</span> <span class=o>=</span> <span class=n>restrict</span><span class=p>(</span><span class=n>coarse</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>q</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>coarse</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>smooth_laplace</span><span class=p>(</span><span class=n>coarse</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>coarse</span> <span class=o>=</span> <span class=n>condition</span><span class=p>(</span><span class=n>coarse</span><span class=p>,</span><span class=n>masks</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span><span class=n>r</span><span class=o>=</span><span class=n>r</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>d</span><span class=o>-</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>coarse</span> <span class=o>=</span> <span class=n>prolong</span><span class=p>(</span><span class=n>coarse</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>coarse</span> <span class=o>=</span> <span class=n>condition</span><span class=p>(</span><span class=n>coarse</span><span class=p>,</span><span class=n>masks</span><span class=p>[</span><span class=o>-</span><span class=n>i</span><span class=o>-</span><span class=mi>2</span><span class=p>],</span><span class=n>r</span><span class=o>=</span><span class=n>r</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>ub</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>prolong</span><span class=p>(</span><span class=n>coarse</span><span class=p>)</span>
</span></span></code></pre></div><p>Convergence can then be specified by a tolerance $\gamma$, such that $(f^{k+1} - f^{k})^2 &lt; \gamma$.
The implication being that the mean squared error between convergence iterations will decrease until acceptable.
Running at a tolerance of $10^{-6}$, for at a size of $256^2$, traversing 6 grids, applying 30 smoothing iterations on each grid,
and 50 on the coarsest grid.<br><img loading=lazy src=/finite_differences/2_3_tol_V_256_6_30_50_1e-6.png alt></p><p>With initialisation,<br><img loading=lazy src=/finite_differences/2_3_tol_V_256_6_30_50_1e-6_init.png alt></p><p>With smooth prolongation,<br><img loading=lazy src=/finite_differences/2_3_tol_V_256_6_30_50_1e-6_smooth.png alt></p><p>With initialisation and smooth prolongation,<br><img loading=lazy src=/finite_differences/2_3_tol_V_256_6_30_50_1e-6_init_smooth.png alt></p><h4 id=w-cycles>W-Cycles<a hidden class=anchor aria-hidden=true href=#w-cycles>#</a></h4><p>Another scheme for grid traversal is the W-Cycle, in which instead of traversing upwards the full path,
one:</p><ul><li>traverses upwards half-way</li><li>traverse downward to the coarsest grid</li><li>traverse upwards to the finest grid</li></ul><p><img loading=lazy src=/finite_differences/2_3_tol_W_256_6_30_50_1e-6.png alt></p><p>With initialisation,<br><img loading=lazy src=/finite_differences/2_3_tol_W_256_6_30_50_1e-6_init.png alt></p><p>With smooth prolongation,<br><img loading=lazy src=/finite_differences/2_3_tol_W_256_6_30_50_1e-6_smooth.png alt></p><p>With initialisation and smooth prolongation,<br><img loading=lazy src=/finite_differences/2_3_tol_W_256_6_30_50_1e-6_init_smooth.png alt></p><h3 id=24-a-gpu-coarse-to-fine-approach>2.4 A GPU Coarse-to-Fine Approach<a hidden class=anchor aria-hidden=true href=#24-a-gpu-coarse-to-fine-approach>#</a></h3><h4 id=laplace-iteration-as-convolution>Laplace iteration as convolution<a hidden class=anchor aria-hidden=true href=#laplace-iteration-as-convolution>#</a></h4><p>The laplace iteration can be turned into a kernel used in 2D-convolution, which is very popular in the deep learning
field, and has often support for GPU accellerated versions of the operation to allow for massively
parallel execution across the grid.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>tf</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>           0    | 0.25 | 0            f[i-1,j-1] | f[i,j-1]  | f[i+1,j-1]
</span></span></span><span class=line><span class=cl><span class=s1>           -------------------        ------------------------------------
</span></span></span><span class=line><span class=cl><span class=s1>f[i,j] =   0.25 | 0    | 0.25     *   f[i-1,j]   | f[i,j]    | f[i+1,j]    
</span></span></span><span class=line><span class=cl><span class=s1>           -------------------        ------------------------------------
</span></span></span><span class=line><span class=cl><span class=s1>           0    | 0.25 | 0            f[i-1,j+1] | f[i,j-1]  | f[i+1,j+1]
</span></span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>,</span><span class=mi>3</span><span class=p>,</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>w</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>,:]</span> <span class=o>=</span> <span class=mf>0.25</span>
</span></span><span class=line><span class=cl><span class=n>w</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>,:,</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=mf>0.25</span>
</span></span><span class=line><span class=cl><span class=n>w</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>multi_grid</span><span class=p>[</span><span class=n>i</span><span class=p>][:,</span><span class=n>idx</span><span class=p>:</span><span class=n>idx</span><span class=o>+</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>conv2d</span><span class=p>(</span><span class=n>multi_grid</span><span class=p>[</span><span class=n>i</span><span class=p>][:,</span><span class=n>idx</span><span class=p>:</span><span class=n>idx</span><span class=o>+</span><span class=mi>1</span><span class=p>],</span> <span class=n>w</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></div><h4 id=prolongation-as-transposed-convolution>Prolongation as transposed convolution<a hidden class=anchor aria-hidden=true href=#prolongation-as-transposed-convolution>#</a></h4><p>Likewise, the naive prolongation used prior can be done quite easily as a transposed 2D-convolution.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>tf</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>f[i,j]   | f[i+1,j]        1 | 1
</span></span></span><span class=line><span class=cl><span class=s1>----------------------  =  -----  *  f[i,j]
</span></span></span><span class=line><span class=cl><span class=s1>f[i,j+1] | f[i+1,j+1]      1 | 1
</span></span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>up</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>,</span><span class=mi>2</span><span class=p>,</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>multi_grid</span><span class=p>[</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>][:,</span><span class=mi>0</span><span class=p>:</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>conv_transpose2d</span><span class=p>(</span><span class=n>multi_grid</span><span class=p>[</span><span class=n>i</span><span class=p>][:,</span><span class=n>idx</span><span class=p>:</span><span class=n>idx</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>up</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></div><p>This converges much faster than both V-Cycles and W-Cycles. For a $256^2$ fine grid, at 6 traversals
and a tolerance of $10^{-6}$.<br><img loading=lazy src=/finite_differences/2_4_256_6_1e-6_cpu.png alt></p><p>Extending to a $2048^2$ fine grid with 10 traversals at a tolerance of $10^{-6}$ is done easily.<br><img loading=lazy src=/finite_differences/2_4_2048_10_1e-6_cpu.png alt></p><h2 id=3-wip-poisson-and-helmholtz-equations>3. [WIP] Poisson and Helmholtz Equations<a hidden class=anchor aria-hidden=true href=#3-wip-poisson-and-helmholtz-equations>#</a></h2><h3 id=31-wip-poissons-equation>3.1 [WIP] Poisson&rsquo;s Equation<a hidden class=anchor aria-hidden=true href=#31-wip-poissons-equation>#</a></h3><p>Naive
$$
\begin{aligned}
g &= \nabla^2 f\\
g_{i,j} &= \frac{1}{h^2} \Big( f_{i+1,j} + f_{i-1,j} + f_{i,j+1} + f_{i,j-1} - 4f_{i,j} \Big)\\
f_{i,j} &= \frac{1}{4} \Big( f_{i+1,j} + f_{i-1,j} + f_{i,j+1} + f_{i,j-1} - h^2g_{i,j} \Big)
\end{aligned}
$$</p><p>Multigridding
$$
f^\star = f + e \tag*{(1)}
$$</p><p>$$
\nabla^2 f^\star = g \qquad \Rightarrow \qquad
r = g - \nabla^2 f \tag*{(2)}
$$</p><p>$$
h^2 r_{i,j} = h^2 g_{i,j} - \Big(
f_{i+1,j} + f_{i-1,j} + f_{i,j+1} + f_{i,j-1} - 4f_{i,j}
\Big)
$$</p><p>$$
\nabla^2 f^\star = 0 ,, \Leftrightarrow ,,
\nabla^2 (f + e) = 0 ,, \Leftrightarrow ,,
\nabla^2 e = -\nabla^2 f \qquad \Leftrightarrow \qquad
\nabla^2 e = r \tag*{(3)}
$$</p><p>$$
\begin{aligned}
h^2 r_{i,j} &\approx e_{i+1,j} + e_{i-1,j} + e_{i,j+1} + e_{i,j-1} - 4e_{i,j}\\
\Rightarrow \qquad e^{k+1}_{i,j} &=
\frac{1}{4} \Big(
e^k_{i+1,j} + e^k_{i-1,j} + e^k_{i,j+1} + e^k_{i,j-1} - h^2 r_{i,j} \Big)
\end{aligned}
$$</p><h3 id=32-x-time-dependency-in-the-forcing-term>3.2 <del>[x] Time-Dependency in the Forcing Term</del><a hidden class=anchor aria-hidden=true href=#32-x-time-dependency-in-the-forcing-term>#</a></h3><h3 id=33-x-helmholtz-equation>3.3 <del>[x] Helmholtz Equation</del><a hidden class=anchor aria-hidden=true href=#33-x-helmholtz-equation>#</a></h3><h3 id=34-x-generalized-form-with-time-dependence>3.4 <del>[x] Generalized Form with Time Dependence</del><a hidden class=anchor aria-hidden=true href=#34-x-generalized-form-with-time-dependence>#</a></h3><h2 id=4-x-the-advection-diffusion-equation>4. <del>[x] The Advection-Diffusion Equation</del><a hidden class=anchor aria-hidden=true href=#4-x-the-advection-diffusion-equation>#</a></h2><h3 id=41-x-one-spatial-dimension>4.1 <del>[x] One Spatial Dimension</del><a hidden class=anchor aria-hidden=true href=#41-x-one-spatial-dimension>#</a></h3><h3 id=42-x-two-spatial-dimensions>4.2 <del>[x] Two Spatial Dimensions</del><a hidden class=anchor aria-hidden=true href=#42-x-two-spatial-dimensions>#</a></h3><h2 id=5-x-rayleigh-benard-convection>5. <del>[x] Rayleigh-Benard Convection</del><a hidden class=anchor aria-hidden=true href=#5-x-rayleigh-benard-convection>#</a></h2></div><footer class=post-footer><ul class=post-tags><li><a href=/tags/pde/>pde</a></li><li><a href=/tags/numeric/>numeric</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href>Russel Demos</a></span>
-
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>